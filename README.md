# TDD in data analysis
## A step-by-step tutorial
The previous two posts of this series were about the [business](https://medium.com/@torokagoston/effective-data-science-teams-part-1-9f40c6bff275) and [people](https://medium.com/@torokagoston/effective-data-science-teams-part-2-6af6f7674be9) perspectives of effective data science teams. However without the right tools and processes the recipe does not work. Therefore in the next parts I'm going to discuss some of the tools and processes that I consider important for a data science team. This part is going to be about how to consolidate the practices of data analysis with the practices of software development. The specific process that I'm going to focus on is test-driven development for data analysis. At first I intended this also as a post around stories and conclusions like the previous ones. But since I did not find a good tutorial from scratch to a working model online, I've decided to make one in the scope of this post. So in the following we discuss the reason of TDD for data analysis and then I present a complete step-by-step tutorial on an example project from Kaggle using TDD.
## Why TDD for data analytics
Test-driven development is a popularized by Kent Beck, the father of extreme programming. It helps the development to focus on the required features and helps rapid iteration of versions. The idea is that when you have a new feature that you have to add to the code first add a test that expects that feature to be present and see it FAIL. Then write the quickest solution to PASS the test. Once it was passed take some time to REFACTOR the code. All three parts of the process are equally important and they work best in this order. Take the first step, fail. I remember when some time ago, we were sitting in front of the computer with my colleagues and were looking at why changing the input parameters of a function does not change the output in the expected way although there was a unittest covering exactly that function. After some investigation it turned out that the mentioned unittest I wrote after the function was written actually never failed. The other important thing is to focus on only passing the actual test and not writing code that is intended to pass all possible tests in the future. One of the guidelines behind is 'Keep it simple, stupid' (KISS). Also the refactoring should not mean changing the test logic or completely rewriting the function that passed it, refactor only as much as absolutely needed. Don't forget the code is your enemy.

Now one can say: alright this is nice and true for writing software for production but how does it relate to my practice when I do exploratory data science or when I just try out different things. First, I'm sure you met the situation when something that you calculated in your notebook on jupyter is not working in the next stage of the development or even when you rerun it a second time. Maybe the fields were not matching, maybe the datatypes, or maybe a seed was not fixed. Second, it should sound familiar when the data just drags you down into the rabbithole: you notice that a linear model does not explain enough variance so you try some non-linear ones, then you see also that some missing values can be predicted so you predict those, then you see that maybe the data should be transformed, some string columns should be vectorized other should be levels of an ordinal scale etc. Then 2 weeks into the exploration the manager asks where is the result that you promised for one week ago and you can only explain him what will be your next steps... and you still did not reduce the uncertainty for her and she still does not know if the feature/model is needed at all or we should concentrate on other promising directions. So I think there are serious reasons to pracitce TDD for data analysis. On top of these empirical considerations, there are theoretical ones as well. TDD and scientific methodology. When you create a test and see it failing is basically making a scientific hypothesis that is [falsifiable](https://en.wikipedia.org/wiki/Falsifiability). When you pass the test that is basically equivalent to the statistical testing of your experimental manipulation in a research study. Finally, refactoring is the similar to the funciton of the discussion part of a paper, you turn your experimental manipulation to a theory. 
## Testing Sanity and Insanity
Although software testing has its own jargon, I considered for some time that instead of unittests or integration tests to think about __insanity__ and __sanity__ tests when it comes to data analysis. Testing insanity means that you want to make sure that the function that you wrote is not producing non-sensical results, it gives some result for input A and some other result for input B etc. i.e. it is not insane. However, no client is going to pay for a solution that is guaranteed to be not insane (100% test coverage). This is only a necessary but not the sufficient condition. For that our solution has to be sane. Therefore sanity testing tests some more higher level and even abstract hypothesis, e.g. that your solution (e.g. a neural network) is better than a simple arithmetic mean or that you are not overfitting the training data. These are features which can be translated easily into direct business value. A financial software requires very strong arguments to not stick to an interpretable model and go for a deep learning solution when they can lose money. Also when you are already overfitting the training data when you compare it to the validation sets then imagine the how much difference you are going to see between the expected fit and real for a dataset that was collected in a later batch that you have not seen during design. The way I think about these tests passing sanity tests make software that you sell and passing insanity tests warrants customer satisfaction in the upcoming months and years of using your product. 

Nevertheless as the saying goes, it is often difficult to tell the fine line between a madman and a genius. The same goes for the question of 'should I put this test in the insanity or in the insanity bucket?'. So, at this point it is time to delve into the step-by-step tutorial. I took an example dataset from Kaggle, the [House Prices dataet](https://www.kaggle.com/c/house-prices-advanced-regression-techniques) this is a sufficiently easy and fun data. Just right the pass the imaginary test of 'tutorial on TDD for analysis'. 

[Aaand here is the notebook with the step by step guide](https://nbviewer.jupyter.org/github/agostontorok/tdd_data_analysis/blob/master/TDD%20in%20data%20analysis%20-%20Step-by-step%20tutorial.ipynb#Step-by-step-TDD-in-a-data-science-task)
